---
layout: default
title: 135회-1교시-4번 몬테카를로 트리탐색(Monte Carlo Tree Search)
parent: 📝 기출문제
grand_parent: AI (인공지능)
nav_order: 1352104
permalink: /docs/ai/exam/135-1-4-mcts
---

# 몬테카를로 트리탐색(Monte Carlo Tree Search)
{: .no_toc }

135회 정보관리기술사 1교시 4번
{: .label .label-blue }

1교시형 (단답형)
{: .label .label-red }

---

## 📚 목차
{: .no_toc .text-delta }

- [🎓 고딩 수준 설명](#-고딩-수준-설명)
- [🎯 기술사 수준 설명](#-기술사-수준-설명)

---

<details markdown="1">
<summary><h2 style="display:inline" id="-고딩-수준-설명">🎓 고딩 수준 설명 (클릭해서 펼치기)</h2></summary>

### 🔑 핵심 키워드 3개

| 키워드 | 설명 |
|:------|:-----|
| **시뮬레이션** | 무작위로 게임을 끝까지 플레이해보는 것 |
| **트리 탐색** | 가능한 수를 나무 가지처럼 펼쳐서 탐색 |
| **역전파** | 시뮬레이션 결과를 부모 노드로 전달 |

---

### 📖 등장배경

```
🌐 과거: 바둑처럼 경우의 수가 너무 많은 게임은 전수 조사가 불가능해요

💡 문제: "모든 경우를 다 계산하지 않고 좋은 수를 찾을 수 없을까?"

✅ 해결: MCTS로 시뮬레이션 기반 탐색!
   → 무작위로 여러 번 시뮬레이션
   → 승률이 높은 방향을 더 탐색
   → 알파고가 이 방식으로 바둑 정복!
```

---

### 📝 개념 정의

> **MCTS(Monte Carlo Tree Search)**: 무작위 시뮬레이션을 통해 효율적으로 트리 경로를 탐색하여 최적의 행동을 결정하는 알고리즘

| 개념 | 쉬운 비유 |
|:-----|:---------|
| **MCTS** | 여러 갈림길에서 조금씩 가보고, 좋아 보이는 길을 더 탐험하는 것 |
| **시뮬레이션** | 각 길을 끝까지 가보고 결과를 확인하는 것 |

---

### 🏗️ 기술요소 (2그룹 × 4개)

#### 그룹 1: 동작 절차 `선확시역`

| 단계 | 설명 |
|:-----|:-----|
| **선**택 | 가장 유망한 노드 선택 |
| **확**장 | 새로운 자식 노드 생성 |
| **시**뮬레이션 | 무작위로 끝까지 플레이 |
| **역**전파 | 결과를 부모로 전달 |

#### 그룹 2: 핵심 요소 `탐활정가`

| 요소 | 설명 |
|:-----|:-----|
| **탐**색(Exploration) | 새로운 경로 탐색 |
| **활**용(Exploitation) | 좋은 경로 집중 탐색 |
| **정**책(Policy) | 노드 선택 기준 |
| **가**치(Value) | 승률/보상 값 |

---

### ⭐ 차별점 키워드

**"UCB(Upper Confidence Bounds)"** - 탐색과 활용의 균형을 맞추는 핵심 알고리즘

---

### 🧾 6하원칙 요약

| 항목 | 내용 |
|:-----|:-----|
| **누가** | 게임 AI 개발자, 강화학습 연구자 |
| **언제** | 경우의 수가 많아 전수 조사가 어려울 때 |
| **어디서** | 바둑, 체스, 로보틱스, 의사결정 시스템 |
| **무엇을** | 시뮬레이션 기반 트리 탐색 알고리즘 |
| **어떻게** | 선택→확장→시뮬레이션→역전파 반복 |
| **왜** | 효율적으로 최적의 행동을 찾기 위해 |

</details>

---

## 🎯 기술사 수준 설명
{: #-기술사-수준-설명}

### 📌 핵심 암기 (Quick Reference)

{: .highlight }
> **MCTS**: 시뮬레이션, 트리탐색, 확률적최적화 → 효율적 의사결정 알고리즘
> - (절차) `선확시역` - 선택, 확장, 시뮬레이션, 역전파
> - (핵심) `탐활` - 탐색(Exploration), 활용(Exploitation)
> - (정책) `트디베` - TreePolicy, DefaultPolicy, BestChildSelection
> - (비교) `방분장단` - 방식, 분야, 장점, 단점

---

### 🔑 핵심 키워드 3개

| 키워드 | 설명 |
|:------|:-----|
| **UCB(Upper Confidence Bounds)** | 탐색과 활용의 균형을 위한 공식 |
| **Tree Policy** | 트리 내에서 노드를 선택하는 정책 |
| **Default Policy** | 시뮬레이션(플레이아웃)을 수행하는 정책 |

---

### 📖 등장배경

```
🌐 과거: 전수 조사 등의 시간 제약적 문제 발생

💡 문제: "바둑처럼 경우의 수가 10^170인 게임은 어떻게 탐색하지?"

✅ 해결: MCTS로 확률적 탐색!
   → 모든 노드를 탐색하는 대신 시뮬레이션 활용
   → 가능성이 높은 방향으로 집중 탐색
   → 알파고가 MCTS + 강화학습으로 바둑 정복!
```

---

### 📝 개념 정의

| 구분 | 정의 (22자 이내) |
|:-----|:----------------|
| **MCTS** | 무작위 시뮬레이션 기반 트리 탐색 |
| **Tree Policy** | 트리 내 노드 선택 정책 |
| **Default Policy** | 시뮬레이션 수행 정책 |

---

### 🔄 MCTS 개념도 (ASCII)

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    MCTS(Monte Carlo Tree Search) 동작 절차                    │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│     ┌────────┐      ┌────────┐      ┌────────────┐      ┌────────┐          │
│     │  선택   │─────►│  확장   │─────►│ 시뮬레이션  │─────►│ 역전파  │          │
│     │Selection│      │Expansion│      │ Simulation │      │Backprop│          │
│     └────────┘      └────────┘      └────────────┘      └────────┘          │
│          │               │                │                  │              │
│          ▼               ▼                ▼                  ▼              │
│       ┌─●─┐           ┌─●─┐           ┌─●─┐              ┌─●─┐             │
│      ╱     ╲         ╱     ╲         ╱     ╲            ╱     ╲            │
│    ●         ●     ●         ●     ●         ●        ●         ●           │
│   ╱ ╲            ╱ ╲   ⬅NEW ╱ ╲              ╱ ╲       ╱↑╲                  │
│  ●   ●          ●   ●       ●   ●            ●   ●     ●↑  ●                │
│       ⬅선택         확장─────┘              ↓무작위    ↑결과전달              │
│                                            플레이아웃                         │
│     [Tree Policy]           [Default Policy]                                │
│                                                                              │
│   • 선택 → 확장 → 시뮬레이션 → 역전파 과정의 반복을 통한 정확도 개선            │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

### 🏗️ 구성요소

#### 그룹 1: 동작 절차 `선확시역`

| 절차 | 영문 | 설명 |
|:-----|:-----|:-----|
| **선**택 | Selection | Root node에서 시작하여 가장 유망한 자식을 선택하며 트리를 탐색 |
| **확**장 | Expansion | 선택된 노드에서 아직 방문하지 않은 자식 노드를 하나 추가 |
| **시**뮬레이션 | Simulation | 새 노드에서 무작위 플레이아웃을 수행하여 결과를 얻음 |
| **역**전파 | Backpropagation | 시뮬레이션 결과를 부모 노드로 전달하여 승률을 업데이트 |

---

#### 그룹 2: 핵심 요소 `탐활`

| 구분 | 내용 | 설명 |
|:-----|:-----|:-----|
| **탐**색 | Exploration | 정책적으로 트리폭 제한하여 확장 시 높은 확률을 예측 |
| **활**용 | Exploitation | 가치판단으로 깊이 제한하여 관심부여 및 선택 |

---

#### 그룹 3: 정책 유형 `트디베`

| 정책 | 설명 |
|:-----|:-----|
| **T**ree Policy | 트리 내에서 노드를 선택하고 확장하는 정책 |
| **D**efault Policy | 시뮬레이션(플레이아웃)을 수행하는 정책 |
| **B**est Child Selection | UCB 공식으로 최적 자식 노드를 선택 |

---

### 📋 UCB(Upper Confidence Bounds) 알고리즘

{: .important }
> **UCB 공식**: UCB = X̄ᵢ + C × √(ln N / nᵢ)
> - X̄ᵢ: 노드 i의 평균 보상 (활용 항)
> - C: 탐색 상수 (탐색과 활용의 균형 조절)
> - N: 부모 노드 방문 횟수
> - nᵢ: 노드 i 방문 횟수
> - 탐색(exploration)과 활용(exploitation)의 균형을 맞추는 핵심 알고리즘

---

### 📊 MCTS vs 탐욕 알고리즘 vs 동적 계획법 비교

| 구분 | MCTS | 탐욕 알고리즘 | 동적 계획법 |
|:-----|:-----|:-------------|:-----------|
| **방식** | 시뮬레이션 및 최적해 탐색 | 매 단계 최적 선택 | 하위문제 분할화 및 결과 재사용 |
| **분야** | 게임 AI, 로보틱스 | 최단경로, 배낭문제 | 최적경로, 문자열문제 |
| **장점** | 불완전 정보도 활용가능, 시간-정확성 절충 | 간단, 고속 | 도메인 내 최적해 도출 |
| **단점** | 근사해 도출, 고성능 요구 | 전역 최적해 미보장 | 복잡, 저속, 깊이↑ 부하↑ |

---

### ⭐ 차별점 키워드 (가산점 포인트)

{: .important }
> **탐색(Exploration)과 활용(Exploitation)의 균형**
> - 난수 사용하여 무작위 샘플링으로 전체 트리를 근사
> - 높은 승률의 행동을 가중치 부여 및 수행
> - UCB(Upper Confidence Bounds) 알고리즘으로 균형 조절
> - 알파고는 MCTS와 정책최적화 강화학습 통해 구현

---

### 📈 상위 토픽 계층도

```
탐색 알고리즘 (Search Algorithms)
├── 완전 탐색
│   ├── DFS (깊이 우선 탐색)
│   ├── BFS (너비 우선 탐색)
│   └── 전수 조사
├── 휴리스틱 탐색
│   ├── A* 알고리즘
│   ├── 탐욕 알고리즘 (Greedy)
│   └── 동적 계획법 (DP)
└── 확률적 탐색
    └── MCTS (Monte Carlo Tree Search) ⭐
        ├── 선택 (Selection)
        ├── 확장 (Expansion)
        ├── 시뮬레이션 (Simulation)
        └── 역전파 (Backpropagation)
        
강화학습 활용
├── AlphaGo = MCTS + Policy Network + Value Network
├── AlphaZero = MCTS + Deep RL (자가 학습)
└── MuZero = MCTS + Model-based RL
```

---

### 🔗 활용 분야

| 분야 | 활용 예시 |
|:-----|:---------|
| **게임 AI** | 바둑(AlphaGo), 체스, 포커 |
| **로보틱스** | 경로 계획, 조작 계획 |
| **의사결정** | 불확실한 환경에서의 최적 선택 |
| **자율주행** | 경로 탐색 및 의사결정 |
| **NPC AI** | 게임 내 지능형 캐릭터 행동 |

---

### ✅ 학습 체크리스트

- [ ] MCTS의 정의 (무작위 시뮬레이션 기반 트리 탐색) 설명 가능
- [ ] 동작 절차 `선확시역` 암기 (선택, 확장, 시뮬레이션, 역전파)
- [ ] 각 단계별 세부 동작 설명 가능
- [ ] 핵심 요소 `탐활` 설명 가능 (탐색, 활용)
- [ ] UCB 알고리즘의 역할 설명 가능
- [ ] MCTS vs 탐욕 알고리즘 vs 동적 계획법 비교 설명 가능
- [ ] Tree Policy vs Default Policy 구분 가능
- [ ] AlphaGo에서 MCTS의 역할 설명 가능
