---
layout: default
title: 137회-1교시-7번 트랜스포머(Transformer)와 MoE(Mixture of Experts)
parent: 📝 기출문제
grand_parent: AI (인공지능)
nav_order: 1371107
permalink: /docs/ai/exam/137-1-7-moe
---

# 트랜스포머(Transformer)와 MoE(Mixture of Experts)
{: .no_toc }

137회 정보관리기술사 1교시 7번
{: .label .label-blue }

1교시형 (단답형) - 비교형
{: .label .label-red }

---

## ✅ 문제

트랜스포머(Transformer)와 MoE(Mixture of Experts)를 설명하시오

---

## 📚 목차
{: .no_toc .text-delta }

- [🎓 고딩 수준 설명](#-고딩-수준-설명)
- [🎯 기술사 수준 설명](#-기술사-수준-설명)

---

<details markdown="1">
<summary><h2 style="display:inline" id="-고딩-수준-설명">🎓 고딩 수준 설명 (클릭해서 펼치기)</h2></summary>

### 🔑 핵심 키워드 3개

| 키워드 | 설명 |
|:------|:-----|
| **Self-Attention** | 문장 속 단어들이 서로 얼마나 관련 있는지 계산하는 기술 |
| **전문가 모델** | 특정 분야를 잘 아는 여러 AI 모델들 |
| **게이팅 네트워크** | 어떤 전문가에게 질문할지 결정하는 라우터 |

### 📖 등장배경

| 구분 | 내용 |
|:-----|:-----|
| **과거** | RNN은 단어를 하나씩 순서대로 처리해서 너무 느렸어요 |
| **문제** | 긴 문장에서 앞부분 내용을 잊어버리고, 병렬처리도 안 돼요! |

### 📝 개념 정의

| 개념 | 정의 | 쉬운 비유 |
|:-----|:-----|:---------|
| **트랜스포머** | Self-Attention으로 문장의 모든 단어 관계를 동시에 파악하는 AI 구조 | 교실에서 모든 학생이 동시에 서로 대화하며 정보 교환 |
| **MoE** | 여러 전문가 중 적합한 전문가만 골라 답변하는 효율적인 AI 구조 | 병원에서 증상에 따라 해당 전문의에게만 진료받는 것 |

### 🏗️ 기술요소 (2그룹 × 4개)

#### 그룹 1: 트랜스포머 핵심 `셀멀포피`

| 요소 | 설명 |
|:-----|:-----|
| **셀**프 어텐션 | 단어 간 관계 파악 |
| **멀**티헤드 어텐션 | 여러 관점에서 동시 분석 |
| **포**지셔널 인코딩 | 단어 위치 정보 부여 |
| **피**드포워드 | 비선형 변환 수행 |

#### 그룹 2: MoE 핵심 `게전스로`

| 요소 | 설명 |
|:-----|:-----|
| **게**이팅 네트워크 | 전문가 선택 라우터 |
| **전**문가 네트워크 | 특화된 서브 모델들 |
| **스**파스 활성화 | 일부만 활성화로 효율 ↑ |
| **로**스 밸런서 | 전문가 간 균형 조절 |

### ⭐ 차별점 키워드

**"희소 활성화(Sparse Activation)"** - MoE는 전체가 아닌 일부 전문가만 활성화하여 연산 효율성을 극대화

### 🧾 6하원칙 요약

| 항목 | 트랜스포머 | MoE |
|:-----|:----------|:----|
| **누가** | Google (Vaswani et al., 2017) | Google (Switch Transformer 등) |
| **언제** | 2017년 "Attention is All You Need" | 2022년 이후 대규모 LLM에 적용 |
| **어디서** | NLP, Vision, 생성AI 전반 | 초거대 언어모델 (GPT-4, Mixtral) |
| **무엇을** | Self-Attention 기반 병렬처리 | 전문가 선택적 활성화 |
| **어떻게** | Q, K, V 연산으로 가중치 계산 | 게이팅 네트워크로 라우팅 |
| **왜** | RNN 순차처리 한계 극복 | 계산량 증가 문제 해결 |

</details>

---

## 🎯 기술사 수준 설명
{: #-기술사-수준-설명}

### 📌 핵심 암기 (Quick Reference)

{: .highlight }
> **Transformer & MoE**: Self-Attention, 병렬처리, 희소활성화 → 대규모 언어모델 핵심 아키텍처
> - (Transformer) `셀멀포피` - 셀프어텐션, 멀티헤드, 포지셔널인코딩, 피드포워드
> - (MoE) `게전스로` - 게이팅네트워크, 전문가네트워크, 스파스활성화, 로스밸런서
> - (비교) `연주활병로장단대` - 연산방식, 주요구성, 활성화, 병렬처리, 로스밸런싱, 장점, 단점, 대표모델

---

<div class="exam-concept-block" markdown="1">

## 🧠 개념 영역


### 🔑 핵심 키워드 3개

| 키워드 | 설명 | 예시 |
|:------|:-----|:-----|
| **Self-Attention** | 입력 시퀀스 내 단어들의 상호 관계를 Query, Key, Value 통해 계산 | 멀티헤드 어텐션 |
| **병렬 처리** | RNN과 달리 순차처리 없이 GPU 병렬화 용이 | Transformer 기반 모델 |
| **희소 활성화** | 전체 Expert 중 일부만 활성화하여 연산 비용 절감 | Top-K Gating |

---

### 📖 등장배경

| 구분 | 내용 |
|:-----|:-----|
| **기술환경 변화** | RNN의 시퀀셜 처리 및 장기의존성 한계 → **Self-Attention 기반 병렬처리** 필요 |
| **비즈니스 요구** | 대규모 LLM 확장 시 계산량 급증 → **효율적인 모델 확장 방법** (MoE) 필요 |

---

### 📝 개념 정의

| 구분 | 정의 (22자 이내) |
|:-----|:----------------|
| **Transformer** | Self-Attention 기반 인코더-디코더 신경망 |
| **MoE** | 게이팅 네트워크로 전문가 선택 활성화 기술 |

---

</div>

## 🏗️ 기술 영역

<div class="exam-tech-block" markdown="1">

### 🏗️ 구성요소 (핵심 암기법 상세) `셀멀포피` / `게전스로`

#### 그룹 1: Transformer 구성요소 `셀멀포피`

| 암기 | 항목 | 설명 | 예시 |
|:-----|:-----|:-----|:-----|
| **셀** | 셀프 어텐션 | Q, K, V 연산으로 Attention Score 계산, 입력 토큰 병렬 처리 | 멀티헤드 셀프 어텐션 |
| **멀** | 멀티헤드 어텐션 | Self-Attention을 병렬로 동시 수행, 각 Head마다 다른 정보에 집중 | 8개 Head 병렬 연산 |
| **포** | 포지셔널 인코딩 | 사인/코사인 함수로 단어 위치 정보 벡터 추가 | sin(pos/10000^(2i/d)) |
| **피** | 피드포워드 NN | ReLU 사용, 비선형성 특징 추출, Position-Wise 완전 연결망 | FFN(x) = max(0, xW₁+b₁)W₂+b₂ |

#### 그룹 2: MoE 구성요소 `게전스로`

| 암기 | 항목 | 설명 | 예시 |
|:-----|:-----|:-----|:-----|
| **게** | 게이팅 네트워크 | 입력 분석하여 어떤 전문가 활성화할지 결정, Softmax/TOP-K Gating | Top-2 Gating |
| **전** | 전문가 네트워크 | 여러 개의 서브 모델(FFN)로 구성, 각각 특정 유형 입력 처리 학습 | 8~64개 Expert |
| **스** | 스파스 활성화 | 전체 전문가 중 일부만 활성화(희소 활성화)로 연산 효율성 향상 | Top-K Expert만 연산 |
| **로** | 로스 밸런서 | 전문가 간 균형있는 분배를 위한 보조 손실(Auxiliary Loss) 추가 | Load Balancing Loss |

---

</div>
<div class="exam-bonus-block" markdown="1">

## ⭐ 차별점 키워드 (가산점 포인트)

{: .important }
> **희소 활성화(Sparse Activation)와 전문가 특화(Expert Specialization)**
> - MoE는 전체 파라미터 수는 많지만 실제 연산량은 상대적으로 낮음
> - Mixtral 8x7B: 총 47B 파라미터 중 추론 시 13B만 활성화
> - MoA(Mixture-of-Agents), CoE(Chain of Expert)와 연계하여 LLM 성능 향상

---

</div>

<details markdown="1">
<summary><h3 style="display:inline">🧠 상세 설명 (클릭해서 펼치기)</h3></summary>

#### 📖 등장배경 상세

```
🌐 과거: RNN 모델의 시퀀셜 처리 및 장기의존성 한계

💡 문제: "병렬화 불가, 긴 문장의 앞부분 정보 손실, 학습 속도 저하"

✅ 해결: Transformer의 Self-Attention으로 전체 시퀀스 동시 처리!
   → 모델 확장에 따른 계산량 급증 문제 발생
   → MoE의 Sparse Activation으로 효율적 대규모 모델 확장
   → Switch Transformer, Mixtral 등 실용화
```

---

#### 🔄 Transformer 구조도 (ASCII)

```
┌─────────────────────────────────────────────────────────────────────┐
│                         Transformer Architecture                      │
├────────────────────────────┬────────────────────────────────────────┤
│        ENCODER (Nx)        │              DECODER (Nx)              │
├────────────────────────────┼────────────────────────────────────────┤
│                            │         Output Probabilities           │
│                            │               ↑                        │
│                            │           [Softmax]                    │
│                            │               ↑                        │
│                            │        [Linear Layer]                  │
│                            │               ↑                        │
│   ┌──────────────────┐     │     ┌────────────────────┐             │
│   │   Add & Norm     │     │     │    Add & Norm      │             │
│   │  Feed Forward    │     │     │   Feed Forward     │             │
│   │   Add & Norm     │     │     │    Add & Norm      │             │
│   │ Multi-Head Attn  │────────→  │ Encoder-Decoder    │             │
│   │   (Self-Attn)    │     │     │    Attention       │             │
│   └────────┬─────────┘     │     │    Add & Norm      │             │
│            │               │     │ Masked Multi-Head  │             │
│            │               │     │    Self-Attn       │             │
│            │               │     └────────┬───────────┘             │
├────────────┼───────────────┼──────────────┼─────────────────────────┤
│  Positional Encoding       │        Positional Encoding             │
│       ⊕                    │              ⊕                         │
│  [Input Embedding]         │       [Output Embedding]               │
│       ↑                    │              ↑                         │
│    Inputs                  │       Outputs (shifted right)          │
└────────────────────────────┴────────────────────────────────────────┘
```

---

#### 🔄 MoE 구조도 (ASCII)

```
┌─────────────────────────────────────────────────────────────┐
│                       MoE Layer                              │
│                                                              │
│                    [Weighted Sum (+)]                        │
│                   ╱      │      ╲                            │
│              G(x)₁    G(x)₂   G(x)ₙ  ← 가중치(Softmax)       │
│                 ×        ×        ×                          │
│    ┌──────────┬──────────┬──────────┬─────┬──────────┐      │
│    │ Expert 1 │ Expert 2 │ Expert 3 │ ... │ Expert n │      │
│    │  (FFN)   │  (FFN)   │  (FFN)   │     │  (FFN)   │      │
│    └────┬─────┴────┬─────┴────┬─────┴──┬──┴────┬─────┘      │
│         │          │          │        │       │             │
│         └──────────┴────┬─────┴────────┴───────┘             │
│                         │                                     │
│                 [Gating Network]                              │
│                    (Router)                                   │
│                         ↑                                     │
│                    Input (x)                                  │
└─────────────────────────────────────────────────────────────┘
```

---

#### 📊 Attention 수식

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

| 기호 | 설명 |
|:-----|:-----|
| Q (Query) | 찾고자 하는 정보 |
| K (Key) | 비교 대상 정보 |
| V (Value) | 실제 전달할 정보 |
| \(d_k\) | Key 벡터의 차원 (스케일링 팩터) |

---

#### 📈 상위 토픽 계층도

```
대규모 언어모델 (LLM) 아키텍처
├── 기반 아키텍처
│   ├── Transformer ⭐
│   │   ├── Encoder-only (BERT)
│   │   ├── Decoder-only (GPT)
│   │   └── Encoder-Decoder (T5)
│   └── 효율화 기법
│       ├── MoE ⭐
│       │   ├── Switch Transformer
│       │   ├── Mixtral
│       │   └── GLaM
│       ├── 양자화 (Quantization)
│       └── 지식증류 (Knowledge Distillation)
└── 활용 기술
    ├── Fine-tuning
    ├── RAG
    └── Prompt Engineering
```

---

#### ✅ 학습 체크리스트

- [ ] Transformer 정의 22자로 말할 수 있다
- [ ] MoE 정의 22자로 말할 수 있다
- [ ] Transformer 구성요소 `셀멀포피` 4요소 설명 가능
- [ ] MoE 구성요소 `게전스로` 4요소 설명 가능
- [ ] Self-Attention 연산 과정 (Q, K, V) 설명 가능
- [ ] Positional Encoding에서 사인/코사인 함수 사용 이유 설명 가능
- [ ] MoE의 Gating Network 동작 원리 설명 가능
- [ ] Sparse Activation의 연산 효율성 장점 설명 가능
- [ ] Transformer vs MoE 비교 `연주활병로장단대` 8가지 암기
- [ ] 대표 모델 예시 (BERT, GPT, Mixtral, Switch Transformer) 연관 설명 가능

</details>

---



<details markdown="1">
<summary><h3 style="display:inline">🧠 상세 설명 (클릭해서 펼치기)</h3></summary>

### 📊 Transformer vs MoE 비교 `연주활병로장단대`

| 암기 | 구분 | Transformer | MoE |
|:-----|:-----|:------------|:----|
| **연** | 연산 방식 | **모든 레이어 사용** (Dense) | **일부 전문가만 활성화** (Sparse Expert) |
| **주** | 주요 구성요소 | Self-Attention, Positional Encoding | Gating Network, Sparse Dispatcher |
| **활** | 활성화 방식 | Fixed (고정된 Layer 사용) | **동적으로 Expert 선택** |
| **병** | 병렬처리 가능성 | 제한적 (모든 Layer 종속적 동작) | Expert들이 독립적, **분산처리 가능** |
| **로** | 로스밸런싱 | 고려하지 않음 | **로스밸런싱 고려 활용** |
| **장** | 장점 | 학습 성능 우수, 범용성 | **연산 효율성, 확장성** |
| **단** | 단점 | 높은 연산 비용, **계산량 증가** | 라우터 학습 복잡성, **Expert 선택 불균형** |
| **대** | 대표 모델 | BERT, T5, GPT-3 | **Mixtral, Switch Transformer, GLaM** |

---

</details>
## 참고(출처/메모)

- Vaswani et al., "Attention is All You Need" (2017)
- Google Switch Transformer, Mixtral 8x7B
