---
layout: default
title: 135회-1교시-2번 Multimodal LLM(Large Language Model)
parent: 📝 기출문제
grand_parent: AI (인공지능)
nav_order: 1351102
permalink: /docs/ai/exam/135-1-2-multimodal-llm
---

# Multimodal LLM(Large Language Model)
{: .no_toc }

135회 정보관리기술사 1교시 2번
{: .label .label-blue }

1교시형 (단답형)
{: .label .label-red }

---

## ✅ 문제

Multimodal LLM(Large Language Model)

---

## 📚 목차
{: .no_toc .text-delta }

- [🎓 고딩 수준 설명](#-고딩-수준-설명)
- [🎯 기술사 수준 설명](#-기술사-수준-설명)

---

<details markdown="1">
<summary><h2 style="display:inline" id="-고딩-수준-설명">🎓 고딩 수준 설명 (클릭해서 펼치기)</h2></summary>

### 🔑 핵심 키워드 3개
- **멀티모달(Multimodal)**: 텍스트·이미지·오디오·영상 등 여러 입력을 함께 처리
- **공통 표현 공간**: 서로 다른 모달을 같은 벡터 공간에 맞춰 의미를 연결
- **융합(Fusion)**: 모달을 합쳐서 더 정확히 이해/추론/생성

### 📖 등장배경
- **과거**: LLM은 텍스트 중심, 비전/음성 모델은 각각 따로 처리하는 경우가 많았음
- **문제**: 실제 서비스는 “보고(이미지) + 읽고(텍스트) + 듣고(오디오)”를 동시에 이해해야 자연스러운 상호작용이 가능
- **해결**: 여러 모달을 함께 받아 **상황 이해와 생성**을 수행하는 Multimodal LLM이 등장/확산

### 📝 개념 정의
> **Multimodal LLM**: 텍스트뿐 아니라 이미지·오디오·영상 등 다양한 데이터를 동시에 처리·분석하여 상호작용하는 대형 언어 모델(LLM)

### 🧩 기술요소/구성요소 (2그룹 × 4개)

| 그룹 | 세부 항목(4개) |
|:--|:--|
| A (전처리/인코딩) | 텍스트 토큰화, 이미지 해상도 조정, 스펙트로그램 변환(오디오), 영상 벡터 분석 |
| B (융합/모델) | Transformer(언어), Vision Transformer(비전), Wav2Vec(음성), Early/Mid/Late Fusion |

### ⭐ 차별점 키워드
- **크로스 모달 이해(Cross-Modal Understanding)**: 서로 다른 입력의 관계를 파악해 의미 있는 정보를 추출

### 🧾 6하원칙 요약
- **누가(Who)**: Multimodal LLM이
- **언제(When)**: 여러 모달 입력이 함께 주어질 때
- **어디서(Where)**: 텍스트·이미지·오디오·영상에서
- **무엇을(What)**: 정보를 이해/추론/생성하고
- **왜(Why)**: 더 자연스러운 서비스 상호작용을 위해
- **어떻게(How)**: `토해스영`으로 전처리하고 `트비웨융`으로 인코딩·융합

</details>

---

## 🎯 기술사 수준 설명
{: #-기술사-수준-설명}

### 📌 핵심 암기 (Quick Reference)

{: .highlight }
> **Multimodal LLM**: 다중입력, 공통표현공간, 융합 → 멀티모달 이해·생성 LLM
> - (전처리) `토해스영` (텍스트 토큰화·해상도·스펙트로그램·영상벡터)
> - (인코더) `트비웨` (Transformer·ViT·Wav2Vec)
> - (융합) `얼미레` (Early·Mid·Late Fusion)

### 🧠 상세 설명

### 1) 개념/특징

| 구분 | 설명 |
|:--|:--|
| **정의** | Multimodal LLM은 텍스트 외에도 이미지/오디오/영상 등 **서로 다른 모달**을 함께 처리하는 대형 언어 모델 |
| **특징1(멀티모달)** | 다양한 형태(Modal)의 데이터 처리 가능 |
| **특징2(다중 입력)** | 그림+텍스트 등 복수 입력을 한 번에 이해/처리 |
| **특징3(공통 표현 공간)** | 멀티모달 데이터를 공통 벡터 공간에 표현해 연관성을 학습 |

---

### 2) 동작 메커니즘(개요)

멀티모달 입력은 **모달별 인코더/프로젝션**을 통해 LLM이 처리 가능한 형태로 변환되고, LLM의 추론 결과는 **모달별 출력 프로젝션/디코더**를 통해 텍스트/이미지/오디오/영상 등으로 출력될 수 있습니다.

---

### 3) 구성요소/기술요소 (2그룹 × 4개) `토해스영` / `트비웨-얼미레`

#### 그룹 1: 전처리/정규화 `토해스영`

| 항목 | 설명 | 예시 |
|:--|:--|:--|
| **텍스트 토큰화** | 텍스트를 모델이 처리 가능한 단위(Token)로 분해 | BPE/WordPiece |
| **해상도 조정** | 이미지 크기를 일정하게 맞춰 비전 모델 입력으로 변환 | Resize/Normalize |
| **스펙트로그램 변환** | 오디오를 주파수 분포(이미지 형태)로 변환 | Mel-Spectrogram |
| **영상 벡터 분석** | 프레임/시퀀스 특징을 벡터화해 이해 가능하게 변환 | Frame feature/embedding |

#### 그룹 2: 인코더/융합 `트비웨-얼미레`

| 항목 | 설명 | 예시 |
|:--|:--|:--|
| **Transformer** | 텍스트 처리 핵심 아키텍처(Attention 기반) | Attention/PosEnc/FFN |
| **Vision Transformer(ViT)** | CNN 없이 패치 기반으로 이미지 처리 | Patch embedding |
| **Wav2Vec** | 음성 신호를 표현 벡터로 변환 | Speech embedding |
| **Fusion(얼미레)** | 모달 결합 방식: Early/Mid/Late | Concat/Intermediate/Decision |

---

### 4) (예시) 멀티모달 LLM vs 이미지 생성 모델 비교(요지)

| 비교 | 이미지 생성 모델(예: DALL·E/Imagen 계열) | Multimodal LLM |
|:--|:--|:--|
| 목적 | 주로 이미지 생성/편집 중심 | 이해+추론+생성(텍스트 중심 + 멀티모달 확장) |
| 입력/출력 | 텍스트→이미지(중심) | 텍스트/이미지/오디오/영상 입력을 통합 처리 |
| 핵심 | 확산/인코더 결합 등 | 공통 표현 공간 + 융합 + LLM 추론 |

---

### 5) 한계 및 과제

| 한계 | 과제 | 설명 |
|:--|:--|:--|
| **연산 비용** | 모델 경량화 | 파라미터/연산 최적화(프루닝/양자화 등) |
| **딥페이크** | xAI 기술 발전 | 합성/조작 콘텐츠 판별·추적 |
| **윤리 문제** | 윤리 가이드라인 | 편향/저작권/유해성 대응 규범 정립 |

---

## 참고(출처/메모)

- 멀티모달 LLM: 다중 입력·공통 표현 공간·융합 기반
