---
layout: default
title: ChatGPT
parent: 4. 자연어처리
grand_parent: AI (인공지능)
nav_order: 7
---

<!-- CSV_APPLIED: 기술사_기본필수노트_AI - AI.csv | NO=38 | 중토픽=Chat GPT -->
# ChatGPT
{: .fs-8 }

자연어처리
{: .label .label-purple }

---

## 🎯 기술사 수준 설명

### 📌 핵심 암기 (Quick Reference)

{: .highlight }
> **Chat GPT**: GPT > 3.5 기반 대화에 최적화된 대화전문 인공지능
> - 암기: `감보최`
> - 키워드: `Chat GPT` `최적화` `Action`

---

<div class="exam-concept-block" markdown="1">

## 🧠 개념 영역

### 🔑 핵심 키워드 3개

| 키워드 | 설명 | 예시 |
|:--|:--|:--|
| **Chat GPT** | 핵심 개념/대상 | - |
| **최적화** | 주요 기법/구성요소 | - |
| **Action** | 절차/평가/특징 | - |

---

### 📖 등장배경

| 구분 | 내용 |
|:--|:--|
| **문제/필요성** | GPT > 3.5 기반 대화에 최적화된 대화전문 인공지능 |
| **활용/사례** | - |

---

### 📝 개념 정의

| 구분 | 정의 |
|:--|:--|
| **Chat GPT** | GPT > 3.5 기반 대화에 최적화된 대화전문 인공지능 |

</div>

---

<div class="exam-tech-block" markdown="1">

## 🏗️ 기술 영역

### 구성요소

#### 그룹 1: 기술요소
{: .highlight-purple }

| 암기 | 항목 | 설명 |
|:--|:--|:--|
| **감** | **> RLHF(Reinforcement Learning with Human Feedback)** | - |
| **보** | **>Agent(Action), Environment(Observation), Reward(요기서 휴먼 피드백)** | - |
| **최** | **> PPO(Proximal Policy Optimization)** | 모델없는 강화학습 |


</div>

---

<details markdown="1">
<summary><h3 style="display:inline">📖 상세 설명 (클릭해서 펼치기)</h3></summary>

#### 내용

- GPT > 3.5 기반 대화에 최적화된 대화전문 인공지능
- > 강화학습, 대화연속성

#### 절차

- 감보최
- > Step1 감독정책학습: 학습데이터 샘플링, 행위 라벨링, Fine Tune GPT 3.5
- > Step2 보상정책학습: 보상학습 데이터 샘플링, 순위부여, 보상모델학습
- > Step3 강화학습(최적화): 신규데이터 입력, 결과생성, 보상평가, PPO사용 정책업데이트

#### 기술요소

- > RLHF(Reinforcement Learning with Human Feedback)
- >Agent(Action), Environment(Observation), Reward(요기서 휴먼 피드백)
- > PPO(Proximal Policy Optimization): 모델없는 강화학습

#### 문제점

- 편향성, 상식결여, 패턴의존, 제한적, 개인화부족

</details>

---

<details markdown="1">
<summary><h3 style="display:inline">🗂️ 기존 내용 (백업)</h3></summary>

# ChatGPT
{: .fs-8 }

자연어처리
{: .label .label-purple }

---

## 핵심 키워드

`ChatGPT` `GPT` `대화형 AI` `RLHF` `언어 생성`

---

## 정의/개념

*(이미지 내용 추출 후 작성)*

---

## 학습 체크리스트

- [ ] 개념 이해
- [ ] 핵심 키워드 암기
- [ ] 실무 적용 사례 파악


</details>

