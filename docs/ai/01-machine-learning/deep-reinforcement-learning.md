---
layout: default
title: ì‹¬ì¸µê°•í™”í•™ìŠµ
parent: 1. ê¸°ê³„í•™ìŠµ
grand_parent: AI (ì¸ê³µì§€ëŠ¥)
nav_order: 23
---

# ì‹¬ì¸µê°•í™”í•™ìŠµ(Deep Reinforcement Learning)
{: .fs-8 }

1.4 ê°•í™”í•™ìŠµ
{: .label .label-yellow }

---

## ğŸ¯ ê¸°ìˆ ì‚¬ ìˆ˜ì¤€ ì„¤ëª…

### ğŸ“Œ í•µì‹¬ ì•”ê¸° (Quick Reference)

{: .highlight }
> **ì‹¬ì¸µê°•í™”í•™ìŠµ**: ê°•í™”í•™ìŠµ ì •ì±… ë˜ëŠ” ê°€ì¹˜í•¨ìˆ˜ë¥¼ ì‹¬ì¸µì‹ ê²½ë§ìœ¼ë¡œ êµ¬ì„± AI í•™ìŠµ ê¸°ë²•
> - (í‚¤ì›Œë“œ) DNN, MDP, Q-learning
> - â­ **ì°¨ë³„ì **: [TODO: ë‹¤ë¥¸ ê¸°ìˆ ê³¼ êµ¬ë³„ë˜ëŠ” í•µì‹¬ íŠ¹ì§•]

---
## í•µì‹¬ í‚¤ì›Œë“œ

`DNN` `MDP` `Q-learning` `Environment` `State` `Agent` `Action` `Reward`

---

## ì •ì˜/ê°œë…

ê°•í™”í•™ìŠµ ì •ì±… ë˜ëŠ” ê°€ì¹˜í•¨ìˆ˜ë¥¼ ì‹¬ì¸µì‹ ê²½ë§ìœ¼ë¡œ êµ¬ì„± AI í•™ìŠµ ê¸°ë²•

> ì‹¬ì¸µê°•í™”í•™ìŠµì€ ê°•í™”í•™ìŠµê³¼ ì‹¬ì¸µí•™ìŠµ(ë”¥ëŸ¬ë‹)ì„ ê²°í•©í•˜ì—¬, ì‹œí–‰ì°©ì˜¤ë¥¼ í†µí•´ ê²°ì • ë‚´ë¦¬ëŠ” ë°©ë²•ì„ í•™ìŠµ

---

## ê°œë…ë„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ì‹¬ì¸µ ê°•í™” í•™ìŠµ                             â”‚
â”‚                (Deep Reinforcement Learning)                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚      ê°•í™” í•™ìŠµ         â”‚              ë”¥ëŸ¬ë‹                  â”‚
â”‚                       â”‚                                      â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚    â”‚ ì—ì´ì „íŠ¸ â”‚â—€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”‚                     â”‚       â”‚
â”‚    â”‚ (Agent) â”‚ìƒíƒœ    â”‚        â”‚   Input layer       â”‚       â”‚
â”‚    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜(State)â”‚   â•‹    â”‚     â—‹ â—‹ â—‹ â—‹        â”‚       â”‚
â”‚  ê´€ì°°ì •ë³´â”‚  ë³´ìƒ  í–‰ë™ â”‚        â”‚   Hidden layer 1    â”‚       â”‚
â”‚(Observ.)â”‚(Reward)â”‚(Act)â”‚        â”‚     â—‹ â—‹ â—‹ â—‹        â”‚       â”‚
â”‚    Ot   â”‚  Rt    â”‚ At â”‚        â”‚   Hidden layer 2    â”‚       â”‚
â”‚    â†“    â†“        â†“    â”‚        â”‚     â—‹ â—‹ â—‹ â—‹        â”‚       â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚        â”‚   Output layer      â”‚       â”‚
â”‚    â”‚ í™˜ê²½            â”‚ â”‚        â”‚     â—‹ â—‹ â—‹          â”‚       â”‚
â”‚    â”‚ (Environment)  â”‚ â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚    â”‚ í˜„ì¬ìƒíƒœ(Env State)â”‚        â”‚                            â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## êµ¬ì„±ìš”ì†Œ

| êµ¬ë¶„ | êµ¬ì„±ìš”ì†Œ |
|:-----|:---------|
| **ë”¥ëŸ¬ë‹ ì¸¡ë©´** | DNN |
| **ê°•í™”í•™ìŠµ ì¸¡ë©´** | MDP |
| | Q-learning |
| | Environment |
| | State |
| | Agent |
| | Action |
| | Reward |

---

## ì—°ê³„ í† í”½

- [ê°•í™”í•™ìŠµ](/docs/ai/01-machine-learning/reinforcement-learning)
- [Q-Learning](/docs/ai/01-machine-learning/q-learning)
- [DNN](/docs/ai/02-deep-learning/mlp)

---

## í•™ìŠµ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] ì‹¬ì¸µê°•í™”í•™ìŠµì˜ ì •ì˜ ì´í•´
- [ ] ê°•í™”í•™ìŠµ + ë”¥ëŸ¬ë‹ ê²°í•© êµ¬ì¡° íŒŒì•…
- [ ] êµ¬ì„±ìš”ì†Œ(DNN, MDP, Q-learning ë“±) ì•”ê¸°

---

## ì°¸ê³ ìë£Œ

- ì •ë³´ê´€ë¦¬ê¸°ìˆ ì‚¬ AI í•™ìŠµìë£Œ
