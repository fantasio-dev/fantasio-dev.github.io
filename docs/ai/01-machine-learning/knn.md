---
layout: default
title: KNN (K-Nearest Neighbor)
parent: 1. 기계학습
grand_parent: AI (인공지능)
nav_order: 15
---

# KNN (K-Nearest Neighbor)
{: .fs-8 }

기계학습
{: .label .label-blue }

---

## 🎯 기술사 수준 설명

### 📌 핵심 암기 (Quick Reference)

{: .highlight }
> **KNN**: K개의 가장 가까운 이웃 클래스로 분류하거나 예측하는 지도학습
> - (거리측정) 유클리드, 맨해튼, 민코프스키
> - ⭐ **차별점**: 학습 단계 없음 (Lazy Learning), 저장된 데이터로 직접 예측

---

## 핵심 키워드

`K` `이웃` `거리측정` `분류` `Lazy Learning`

---

## 정의/개념

K개의 가장 가까운 이웃 클래스로 분류하거나 예측하는 지도학습

---

## 알고리즘 흐름

```
1. K 값 선택 (예: K=3)
      ↓
2. 새 데이터와 모든 학습 데이터 간 거리 계산
      ↓
3. 거리 기준 K개의 최근접 이웃 선택
      ↓
4. 다수결 투표로 클래스 결정 (분류)
   또는 평균으로 값 결정 (회귀)
```

---

## 거리 측정 방법

| 방법 | 설명 | 수식 |
|:-----|:-----|:-----|
| **유클리드 거리** | 직선 거리 | √Σ(xi-yi)² |
| **맨해튼 거리** | 축 따라 거리 | Σ\|xi-yi\| |
| **민코프스키 거리** | 일반화 | (Σ\|xi-yi\|^p)^(1/p) |

---

## K 값 선택

| K 값 | 특징 |
|:-----|:-----|
| **K가 작음** | 노이즈에 민감, 과적합 위험 |
| **K가 큼** | 결정 경계 부드러움, 과소적합 위험 |
| **일반적** | 홀수 선택 (동점 방지) |

---

## 장단점

| 장점 | 단점 |
|:-----|:-----|
| 단순하고 직관적 | 계산 비용 높음 |
| 학습 단계 없음 | 고차원 데이터에 취약 |
| 비선형 분류 가능 | K 선택 어려움 |

---

## 연계 토픽

- [SVM]({{ site.baseurl }}/docs/ai/01-machine-learning/svm)
- [지도학습]({{ site.baseurl }}/docs/ai/01-machine-learning/supervised-learning)

---

## 학습 체크리스트

- [ ] KNN 정의 암기
- [ ] 거리 측정 방법 3가지 설명
- [ ] K 값 선택 기준 설명
